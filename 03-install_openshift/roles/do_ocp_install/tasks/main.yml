---
- name: Start Cluster bastion and master VMs
  delegate_to: localhost
  become: true
  ignore_errors: true
  virt:
    command: start
    name: "{{ item }}"
  with_items:
    - "{{ groups['ocp4_bootstrap_vm'] }}"
    - "{{ groups['ocp4_master_vm'] }}"

- name: Wait for bootstrap to complete
  shell: |
    cd {{ openshift_installer_dir }}
    /usr/bin/openshift-install wait-for bootstrap-complete --log-level debug
  async: "{{ 45 * 60 }}"

- name: Remove Bootstrap Infrastructure
  become: true
  block:
  - name: Remove Machine Config bootstrap in LB
    lineinfile:
      state: absent
      regexp: "server bootstrap.*22623"
      path: /etc/haproxy/haproxy.cfg
  - name: Remove API bootstrap in LB
    lineinfile:
      state: absent
      regexp: "server bootstrap.*6443"
      path: /etc/haproxy/haproxy.cfg
  - name: Restart load balancer
    service:
      name: haproxy
      state: restarted
  - name: Stop bootstrap VM
    delegate_to: localhost
    ignore_errors: true
    virt:
      command: destroy
      name: "{{ groups['ocp4_bootstrap_vm'][0] }}"
  - name: Undefine bootstrap VM
    delegate_to: localhost
    ignore_errors: true
    virt:
      command: undefine
      name: "{{ groups['ocp4_bootstrap_vm'][0] }}"
  - name: Cleanup bootstrap storage
    delegate_to: localhost
    file:
      path: "{{ vm_files_path }}/{{ groups['ocp4_bootstrap_vm'][0] }}.qcow2"
      state: absent

- name: Make sure .kube directory exists in home directory
  file:
    state: directory
    path: "/home/{{ bastion_cloud_user }}/.kube"
    owner: "{{ bastion_cloud_user }}"
    mode: 0775

- name: Set up .kube/config
  copy:
    remote_src: yes
    src: "{{ openshift_installer_dir }}/auth/kubeconfig"
    dest: "/home/{{ bastion_cloud_user }}/.kube/config"

- name: Make sure .kube directory exists in /root
  file:
    state: directory
    path: /root/.kube
    owner: root
    mode: 0700
  become: yes

- name: Set up .kube/config for root
  copy:
    remote_src: yes
    src: "{{ openshift_installer_dir }}/auth/kubeconfig"
    dest: /root/.kube/config
  become: yes

- name: Start workstation VMs
  delegate_to: localhost
  become: true
  ignore_errors: true
  virt:
    command: start
    name: "{{ item }}"
  with_items:
    - "{{ groups['ocp4_worker_vm'] }}"

- name: Wait for workstation machines to finish booting
  wait_for:
    host: "{{ hostvars[item].vm_ipaddress }}"
    port: 22
  with_items: "{{ groups['ocp4_worker_vm'] }}"

- name: Get CSR signing script
  copy:
    src: sign.sh
    dest: "/home/{{ bastion_cloud_user }}/sign.sh"
    owner: "{{ bastion_cloud_user }}"
    group: "{{ bastion_cloud_user }}"
    mode: 0755

- name: Sign CSRs
  command: "sh sign.sh {{ workers | length }}"

- name: Wait for install to complete
  shell: |
    cd {{ openshift_installer_dir }}
    /usr/bin/openshift-install wait-for install-complete
  async: "{{ 35 * 60 }}"

- name: Add epel-release repo
  package:
    state: present
    name: https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm
  become: true

- name: Install Python Dependencies
  package:
    state: present
    name:
    - python3-openshift
  become: true

- name: Create OpenShift objects for NFS Provisioner - Part 1
  k8s:
    state: present
    merge_type:
    - strategic-merge
    - merge
    definition: "{{ lookup('template', item ) | from_yaml }}"
  loop:
  - ./templates/project.yaml.j2
  - ./templates/nfs-role.yaml.j2
  - ./templates/service-account.yaml.j2
  - ./templates/nfs-role-binding.yaml.j2
  - ./templates/nfs-local-role.yaml.j2
  - ./templates/nfs-local-role-binding.yaml.j2

- name: Grant host mount SCC to NFS provisioner
  command: "oc adm policy add-scc-to-user hostmount-anyuid system:serviceaccount:nfs-client-provisioner:nfs-client-provisioner"

- name: Create OpenShift objects for NFS Provisioner - Part 2
  k8s:
    state: present
    merge_type:
    - strategic-merge
    - merge
    definition: "{{ lookup('template', item ) | from_yaml }}"
  loop:
  - ./templates/nfs-provisioner-deployment.yaml.j2
  - ./templates/storage-class.yaml.j2

- name: Patch image registry storage location - NFS
  shell: |
    oc patch configs.imageregistry.operator.openshift.io cluster \
      --type merge --patch '{"spec":{"storage":{"pvc":{"claim":""}}}}'
    oc patch configs.imageregistry.operator.openshift.io cluster \
      --type merge --patch '{"spec":{"managementState":"Managed"}}'
  when: nfs_registry

- name: Patch image registry storage location - Local (POC only)
  shell: |
    oc patch configs.imageregistry.operator.openshift.io cluster \
      --type merge --patch '{"spec":{"storage":{"emptyDir":{}}}}'
    oc patch configs.imageregistry.operator.openshift.io cluster \
      --type merge --patch '{"spec":{"managementState":"Managed"}}'
  when: not nfs_registry

- name: Give private registry time to cycle
  pause:
    seconds: 15

- name: Wait on registry to be online after storage change
  shell: |
    oc get co image-registry -ojson | \
      jq -r '.status.conditions[] | select(.type == "Available") | select(.status|test("True")) | 1'
  register: status
  until: status.stdout == "1"
  delay: 5
  retries: 5

- name: Allow private registry pods to restart
  pause:
    seconds: 30

- name: Remove worker designation from masters
  shell: |
    oc patch schedulers.config.openshift.io cluster \
      --type merge --patch '{"spec":{"mastersSchedulable":false}}'

# smoke test
- name: Get API for command line
  command: oc whoami --show-server
  register: showserver

- name: Discover web console URI
  when: webconsole is not defined
  block:
    - name: Get console route
      command: oc get route -n openshift-console console -o json
      register: routeconsole
      retries: 10
      delay: 30
      until: routeconsole is succeeded
      ignore_errors: yes

    - name: Set webconsole address
      set_fact:
        webconsole: "http://{{ routeconsole.stdout | from_json | json_query('spec.host') }}"
      when: routeconsole is succeeded

- name: Check DNS webconsole
  command: nslookup "{{ webconsole | urlsplit('hostname') }}"
  register: checkdnswebconsole
  changed_when: false
  retries: 15
  until: checkdnswebconsole is succeeded
  delay: 30

- name: Check DNS API
  command: nslookup "{{ showserver.stdout | trim | urlsplit('hostname') }}"
  register: checkdnsapi
  changed_when: false

- name: Webconsole
  uri:
    url: "{{ webconsole }}"
    validate_certs: no
  register: testwebconsole
  retries: 5
  until: testwebconsole is succeeded
  delay: 60

- name: Cluster-info
  command: oc cluster-info
  register: clusterinfor
  changed_when: false

- name: Create project
  command: oc new-project postflightcheck
  register: newproject

- name: New-app
  command: oc new-app cakephp-mysql-example -n postflightcheck
  register: newapp

- name: Wait for mysql
  command: timeout 300 oc rollout status dc/mysql -w -n postflightcheck
  register: mysqlw
  changed_when: false

- name: Wait for php
  command: timeout 300 oc rollout status dc/cakephp-mysql-example -w -n postflightcheck
  register: phpw
  changed_when: false
  retries: 2
  delay: 60
  until: phpw is succeeded

- name: Get route
  command: >-
    oc get route
    -l template=cakephp-mysql-example
    --no-headers
    -o json
    -n postflightcheck
  register: getroute
  changed_when: false
  retries: 10
  delay: 5
  until: getroute is succeeded

- name: Test that route is reachable
  uri:
    url: "http://{{ getroute.stdout|from_json|json_query('items[0].spec.host') }}"
  register: testroute
  retries: 15
  delay: 5
  until: testroute is succeeded

- name: Delete project
  command: oc delete project postflightcheck

- name: Switch back to default project
  command: oc project default

- debug:
    msg: "{{ item }}"
  loop:
    - "user.info: "
    - "user.info: Post Flight Check"
    - "user.info: DNS Web Console ............... {{ 'OK' if checkdnswebconsole.rc == 0 else 'FAIL' }}"
    - "user.info: DNS API ....................... {{ 'OK' if checkdnsapi.rc == 0 else 'FAIL' }}"
    - "user.info: Web console ................... {{ 'OK' if testwebconsole is succeeded else 'FAIL' }}"
    - "user.info: API ........................... {{ 'OK' if clusterinfor.rc == 0 else 'FAIL' }}"
    - "user.info: Create Project with PV ........ {{ 'OK' if newproject.rc == 0 else 'FAIL' }}"
    - "user.info: App deployed .................. {{ 'OK' if phpw.rc == 0 and mysqlw.rc == 0 else 'FAIL' }}"
    - "user.info: Route ......................... {{ 'OK' if testroute is succeeded else 'FAIL' }}"

- when:
    - openshift_smoke_tests_must_succeed | bool
    - >-
      checkdnswebconsole.rc != 0
      or checkdnsapi.rc != 0
      or testwebconsole is failed
      or clusterinfor.rc != 0
      or newproject.rc != 0
      or phpw.rc != 0
      or mysqlw.rc != 0
      or testroute is failed
  fail:
    msg: "FAIL Smoke tests"
  ignore_errors: no

# configure htpasswd with optional admin/developer user accounts
- name: Configure htpasswd authentication
  when: configure_htpasswd_auth
  block:
  - name: Generate htpasswd hash for user_password
    shell: >-
      htpasswd -nb "{{ developer_user }}" "{{ developer_password }}" | cut -d: -f2
    register: htpasswd_line
    when:
      - developer_user is defined

  - name: Set fact user_password_hash
    set_fact:
      developer_password_hash: "{{ htpasswd_line.stdout }}"
    when:
      - developer_user is defined
      - htpasswd_line is succeeded

  - name: Generate htpasswd hash for admin user
    shell: >-
      htpasswd -nb "{{ admin_user }}" "{{ admin_password }}" | cut -d: -f2
    register: htpasswd_line
    when:
      - admin_user is defined

  - name: Set fact admin_password_hash
    set_fact:
      admin_password_hash: "{{ htpasswd_line.stdout }}"
    when:
      - admin_user is defined
      - htpasswd_line is succeeded

  - name: Generate htpasswd file
    template:
      src: htpasswd.j2
      dest: "/home/{{ bastion_cloud_user }}/users.htpasswd"
      owner: "{{ bastion_cloud_user }}"
      mode: 0664

  - name: Upload OAuth Configuration File
    template:
      src: oauth-htpasswd.yaml
      dest: "/home/{{ bastion_cloud_user }}/oauth-htpasswd.yaml"
      owner: "{{ bastion_cloud_user }}"
      mode: 0664

  - name: Create htpasswd Secret
    shell: |
      oc delete secret htpasswd-secret -n openshift-config
      oc create secret generic htpasswd-secret -n openshift-config --from-file=htpasswd=/home/{{ bastion_cloud_user }}/users.htpasswd

  - name: Update OAuth Configuration
    shell: "oc apply -f /home/{{ bastion_cloud_user }}/oauth-htpasswd.yaml"

  - name: Make admin user a cluster admin
    shell: "oc adm policy add-cluster-role-to-user cluster-admin {{ admin_user }}"
    when: admin_user is defined

  - name: Cleanup auth files on host
    file:
      state: absent
      path: "/home/{{ bastion_cloud_user }}/{{ item }}"
    with_items:
      - oauth-htpasswd.yaml
      - users.htpasswd

# optionally upgrade cluster
- name: Upgrade cluster
  when: openshift_upgrade
  block:
  - name: Upgrade cluster if available
    shell: |
      oc adm upgrade --to-latest=true
  - name: Wait for upgrade to begin
    pause:
      seconds: 20
  - name: Get cluster version info
    shell: |
      oc get clusterversion version -ojson | jq -r '.status.conditions[] | select(.type=="Progressing") | .message'
    register: clusterstatus
  - name: Notify of upgrade status
    debug:
      msg: "Requested upgrade. Status is {{ clusterstatus.stdout }}"
